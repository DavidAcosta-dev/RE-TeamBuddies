#!/usr/bin/env python3
"""Aggregate reverse-engineering call map evidence into a single index.

This script scans Markdown research notes (e.g. vertical/secondary dossiers and
mapping summaries) and emits a consolidated CSV/Markdown pair that lists every
function/address pairing we can find alongside the originating context.

By default it searches a curated set of filename globs that capture the
vertically-oriented analysis notes as well as the mapping exports generated by
previous automation. Provide custom globs to extend or narrow the scope.
"""
from __future__ import annotations

import argparse
import csv
import json
import re
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, Iterable, List, Sequence

REPO_ROOT = Path(__file__).resolve().parents[1]
DEFAULT_PATTERNS: Sequence[str] = (
    "secondary_*.md",
    "vertical_*.md",
    "exports/secondary_*.md",
    "exports/vertical_*.md",
    "exports/mapping_*.md",
    "exports/vertical_*.csv",
    "exports/secondary_*.csv",
)

HEADER_RE = re.compile(r"^(?P<level>#+)\s+(?P<title>.+?)\s*$")
HEADER_FUNCTION_RE = re.compile(
    r"(?P<name>[A-Za-z0-9_]+)\s*(?:@|\(|-)\s*(?P<addr>0x[0-9A-Fa-f]+)"
)
TABLE_SPLIT_RE = re.compile(r"\\|")
ADDRESS_RE = re.compile(r"0x[0-9A-Fa-f]+")


@dataclass
class Record:
    source_file: Path
    category: str
    context: str
    function: str
    address: str
    tags: str = ""
    attributes: Dict[str, str] = field(default_factory=dict)

    def to_csv_row(self) -> Dict[str, str]:
        return {
            "function": self.function,
            "address": self.address,
            "source_file": str(self.source_file).replace("\\", "/"),
            "category": self.category,
            "context": self.context,
            "tags": self.tags,
            "attributes": json.dumps(self.attributes, ensure_ascii=False, sort_keys=True),
        }


def normalize_identifier(text: str) -> str:
    text = text.strip().strip("`")
    return text


def normalize_address(text: str) -> str:
    match = ADDRESS_RE.search(text)
    if match:
        return match.group(0).lower()
    text = text.strip().strip("` ")
    if text.lower().startswith("0x"):
        return text.lower()
    return text


def current_context(headings: Dict[int, str]) -> str:
    if not headings:
        return ""
    ordered = [headings[level] for level in sorted(headings)]
    return " / ".join(ordered)


def parse_markdown(path: Path) -> Iterable[Record]:
    records: List[Record] = []
    headings: Dict[int, str] = {}
    with path.open(encoding="utf-8") as fp:
        lines = fp.read().splitlines()

    idx = 0
    while idx < len(lines):
        line = lines[idx]
        header_match = HEADER_RE.match(line)
        if header_match:
            level = len(header_match.group("level"))
            title = header_match.group("title").strip()
            headings = {lvl: text for lvl, text in headings.items() if lvl < level}
            headings[level] = title
            func_match = HEADER_FUNCTION_RE.search(title)
            if func_match:
                name = normalize_identifier(func_match.group("name"))
                address = normalize_address(func_match.group("addr"))
                if name and address:
                    category = derive_category(path)
                    context = current_context(headings)
                    records.append(
                        Record(
                            source_file=path.relative_to(REPO_ROOT),
                            category=category,
                            context=context,
                            function=name,
                            address=address,
                            attributes={"source": "heading"},
                        )
                    )
            idx += 1
            continue

        if line.startswith("|"):
            table_records, consumed = parse_markdown_table(lines, idx, path, headings)
            records.extend(table_records)
            if consumed:
                idx += consumed
                continue

        idx += 1

    return records


def parse_markdown_table(
    lines: Sequence[str], start_index: int, path: Path, headings: Dict[int, str]
) -> tuple[List[Record], int]:
    header_line = lines[start_index]
    if start_index + 1 >= len(lines):
        return ([], 0)
    separator_line = lines[start_index + 1]
    if not separator_line.startswith("|"):
        return ([], 0)

    headers = [segment.strip() for segment in header_line.strip("|").split("|")]
    if "Function" not in headers and "name" not in headers and "Name" not in headers:
        # Not a function-oriented table.
        return ([], 0)

    records: List[Record] = []
    idx = start_index + 2
    while idx < len(lines):
        row_line = lines[idx]
        if not row_line.startswith("|"):
            break
        cells = [segment.strip() for segment in row_line.strip("|").split("|")]
        if len(cells) < len(headers):
            cells.extend([""] * (len(headers) - len(cells)))
        row = dict(zip(headers, cells))
        function = row.get("Function") or row.get("name") or row.get("Name") or ""
        address = row.get("EA") or row.get("ea") or row.get("Address") or row.get("address") or ""
        if not function or not address:
            idx += 1
            continue
        function = normalize_identifier(function)
        address = normalize_address(address)
        if not function or not address:
            idx += 1
            continue
        tags = row.get("tags") or row.get("Tags") or ""
        attributes = {
            key: value
            for key, value in row.items()
            if key not in {"Function", "name", "Name", "EA", "ea", "Address", "address", "tags", "Tags"}
            and value
        }
        records.append(
            Record(
                source_file=path.relative_to(REPO_ROOT),
                category=derive_category(path),
                context=current_context(headings),
                function=function,
                address=address,
                tags=tags,
                attributes=attributes,
            )
        )
        idx += 1

    consumed = idx - start_index
    return (records, consumed)


def parse_csv(path: Path, headings: Dict[int, str]) -> Iterable[Record]:
    records: List[Record] = []
    with path.open(newline="", encoding="utf-8") as fp:
        reader = csv.DictReader(fp)
        for row in reader:
            function = row.get("Function") or row.get("name") or row.get("Name")
            address = row.get("EA") or row.get("ea") or row.get("Address") or row.get("address")
            if not function or not address:
                continue
            function = normalize_identifier(function)
            address = normalize_address(address)
            tags = row.get("tags") or row.get("Tags") or ""
            attributes = {
                key: value
                for key, value in row.items()
                if key not in {"Function", "name", "Name", "EA", "ea", "Address", "address", "tags", "Tags"}
                and value
            }
            records.append(
                Record(
                    source_file=path.relative_to(REPO_ROOT),
                    category=derive_category(path),
                    context="CSV table",
                    function=function,
                    address=address,
                    tags=tags,
                    attributes=attributes,
                )
            )
    return records


def derive_category(path: Path) -> str:
    stem = path.stem.lower()
    if "vertical" in stem:
        return "vertical"
    if "secondary" in stem:
        return "secondary"
    if "gravity" in stem:
        return "gravity"
    if "mapping" in stem:
        return "mapping"
    return stem.split("_")[0]


def gather_records(patterns: Sequence[str]) -> List[Record]:
    records: List[Record] = []
    seen_keys = set()
    for pattern in patterns:
        for path in sorted(REPO_ROOT.glob(pattern)):
            if not path.is_file():
                continue
            try:
                if path.suffix.lower() == ".md":
                    for record in parse_markdown(path):
                        key = (record.function, record.address, record.source_file)
                        if key in seen_keys:
                            continue
                        seen_keys.add(key)
                        records.append(record)
                elif path.suffix.lower() == ".csv":
                    for record in parse_csv(path, {}):
                        key = (record.function, record.address, record.source_file)
                        if key in seen_keys:
                            continue
                        seen_keys.add(key)
                        records.append(record)
            except UnicodeDecodeError:
                continue
    return records


def write_outputs(records: Sequence[Record], output_dir: Path) -> None:
    output_dir.mkdir(parents=True, exist_ok=True)
    csv_path = output_dir / "function_call_map.csv"
    md_path = output_dir / "function_call_map.md"

    sorted_records = sorted(records, key=lambda rec: (rec.function, rec.address, rec.source_file))

    with csv_path.open("w", newline="", encoding="utf-8") as fp:
        fieldnames = ["function", "address", "source_file", "category", "context", "tags", "attributes"]
        writer = csv.DictWriter(fp, fieldnames=fieldnames)
        writer.writeheader()
        for record in sorted_records:
            writer.writerow(record.to_csv_row())

    with md_path.open("w", encoding="utf-8") as fp:
        fp.write("# Function Call Map Index\n\n")
        fp.write(
            "Generated by `scripts/build_call_map_index.py`. Contains {} unique function/address records.\n\n".format(
                len(sorted_records)
            )
        )
        fp.write("| Function | Address | Category | Context | Source | Tags |\n")
        fp.write("|---|---|---|---|---|---|\n")
        for record in sorted_records[:500]:
            fp.write(
                "| {function} | {address} | {category} | {context} | {source} | {tags} |\n".format(
                    function=record.function,
                    address=record.address,
                    category=record.category,
                    context=record.context.replace("|", "/"),
                    source=str(record.source_file).replace("|", "/"),
                    tags=(record.tags or "").replace("|", "/"),
                )
            )
        if len(sorted_records) > 500:
            fp.write("| … | … | … | … | … | … |\n")

    rel_csv = csv_path.relative_to(REPO_ROOT)
    rel_md = md_path.relative_to(REPO_ROOT)
    print(f"Wrote {rel_csv} and {rel_md}")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Build consolidated function call map index")
    parser.add_argument("--patterns", nargs="*", default=list(DEFAULT_PATTERNS), help="Glob patterns to scan relative to the repo root")
    parser.add_argument(
        "--output-dir",
        default="exports",
        help="Directory (relative or absolute) to place the generated outputs",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    patterns = args.patterns or list(DEFAULT_PATTERNS)
    records = gather_records(patterns)
    output_dir = Path(args.output_dir)
    if not output_dir.is_absolute():
        output_dir = REPO_ROOT / output_dir
    write_outputs(records, output_dir)


if __name__ == "__main__":
    main()
